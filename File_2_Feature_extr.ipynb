{"cells":[{"cell_type":"markdown","metadata":{"id":"PtMVdLHvJlZr"},"source":["# Video Classifcation/Action Recognition using CNN and RNN"]},{"cell_type":"markdown","metadata":{"id":"WzfRhL9yoP2G"},"source":["## Data collection\n","\n","Dataset: UCF101 Action Recognition dataset (https://www.crcv.ucf.edu/data/UCF101.php)\n","\n","In order to keep the data collection simple, we have already downloaded UCF101 dataset split(1) to google drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32740,"status":"ok","timestamp":1642270618117,"user":{"displayName":"CHINMAY INGLE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJoPoknO1Net5LVpEE3dmLs8dT4g8BSLE7zj2H=s64","userId":"10102245642602339447"},"user_tz":-330},"id":"uBhOplXxoP2I","outputId":"c2222ad2-b92d-4afc-dda7-50bdf8ea8ce0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lytsmO9A574R"},"outputs":[],"source":["!cp /content/gdrive/MyDrive/ucf101.tar.gz /content"]},{"cell_type":"code","source":["!tar xf ucf101.tar.gz"],"metadata":{"id":"88upOKOl0pdV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bwQQeLVgoP2J"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vf-fdqqDoP2J"},"outputs":[],"source":["from tensorflow import keras\n","from imutils import paths\n","\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import imageio\n","import cv2\n","import os\n","import pickle\n","import math"]},{"cell_type":"markdown","metadata":{"id":"8K86KmvdoP2K"},"source":["## Define hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SYOnBafioP2L"},"outputs":[],"source":["IMG_SIZE = 224\n","\n","FPS = 6\n","Frames_to_skip = math.floor(25/FPS) - 1  #UCF101 videos are 25fps standardized\n","\n","MAX_FRAMES = 0\n","MAX_SEQ_LENGTH = 20  #1 2 3 *4 5 6 7 *8 9 10 11 *12 13 14 15 *16 17 18 19 *20 21 22 23 *24 25 ---> 3.2 sec of 1 video\n","NUM_FEATURES = 1280"]},{"cell_type":"markdown","metadata":{"id":"d4mHVpvyoP2O"},"source":["## Data preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400},"executionInfo":{"elapsed":445,"status":"ok","timestamp":1642271188106,"user":{"displayName":"CHINMAY INGLE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJoPoknO1Net5LVpEE3dmLs8dT4g8BSLE7zj2H=s64","userId":"10102245642602339447"},"user_tz":-330},"id":"27AS9jjQoP2P","outputId":"5b93946d-db44-413a-8cdd-6e3d30180b5b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total videos for training: 9537\n","Total videos for testing: 3783\n"]},{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-09ee0c33-cf58-4379-85f4-db5841d9ab1e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video_name</th>\n","      <th>tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3826</th>\n","      <td>v_HighJump_g21_c03.avi</td>\n","      <td>HighJump</td>\n","    </tr>\n","    <tr>\n","      <th>669</th>\n","      <td>v_BaseballPitch_g24_c04.avi</td>\n","      <td>BaseballPitch</td>\n","    </tr>\n","    <tr>\n","      <th>7481</th>\n","      <td>v_Shotput_g11_c05.avi</td>\n","      <td>Shotput</td>\n","    </tr>\n","    <tr>\n","      <th>4354</th>\n","      <td>v_JugglingBalls_g10_c02.avi</td>\n","      <td>JugglingBalls</td>\n","    </tr>\n","    <tr>\n","      <th>8546</th>\n","      <td>v_TaiChi_g10_c01.avi</td>\n","      <td>TaiChi</td>\n","    </tr>\n","    <tr>\n","      <th>8171</th>\n","      <td>v_StillRings_g25_c06.avi</td>\n","      <td>StillRings</td>\n","    </tr>\n","    <tr>\n","      <th>7797</th>\n","      <td>v_Skijet_g22_c04.avi</td>\n","      <td>Skijet</td>\n","    </tr>\n","    <tr>\n","      <th>7616</th>\n","      <td>v_SkateBoarding_g20_c02.avi</td>\n","      <td>SkateBoarding</td>\n","    </tr>\n","    <tr>\n","      <th>6265</th>\n","      <td>v_PlayingTabla_g08_c03.avi</td>\n","      <td>PlayingTabla</td>\n","    </tr>\n","    <tr>\n","      <th>4918</th>\n","      <td>v_Lunges_g11_c04.avi</td>\n","      <td>Lunges</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09ee0c33-cf58-4379-85f4-db5841d9ab1e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-09ee0c33-cf58-4379-85f4-db5841d9ab1e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-09ee0c33-cf58-4379-85f4-db5841d9ab1e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                       video_name            tag\n","3826       v_HighJump_g21_c03.avi       HighJump\n","669   v_BaseballPitch_g24_c04.avi  BaseballPitch\n","7481        v_Shotput_g11_c05.avi        Shotput\n","4354  v_JugglingBalls_g10_c02.avi  JugglingBalls\n","8546         v_TaiChi_g10_c01.avi         TaiChi\n","8171     v_StillRings_g25_c06.avi     StillRings\n","7797         v_Skijet_g22_c04.avi         Skijet\n","7616  v_SkateBoarding_g20_c02.avi  SkateBoarding\n","6265   v_PlayingTabla_g08_c03.avi   PlayingTabla\n","4918         v_Lunges_g11_c04.avi         Lunges"]},"metadata":{},"execution_count":7}],"source":["train_df = pd.read_csv(\"train.csv\")\n","test_df = pd.read_csv(\"test.csv\")\n","\n","print(f\"Total videos for training: {len(train_df)}\")\n","print(f\"Total videos for testing: {len(test_df)}\")\n","\n","train_df.sample(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cjdYUAX1oP2R"},"outputs":[],"source":["# Helper functions to load, extract and preprocess frames from video\n","\n","def crop_center_square(frame):\n","    y, x = frame.shape[0:2]\n","    min_dim = min(y, x)\n","    start_x = (x // 2) - (min_dim // 2)\n","    start_y = (y // 2) - (min_dim // 2)\n","    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n","\n","# Returns frames of a video\n","def load_video(path, max_frames=MAX_FRAMES, resize=(IMG_SIZE, IMG_SIZE)):\n","    cap = cv2.VideoCapture(path)\n","    frames = []\n","    try:\n","      \n","        target = Frames_to_skip\n","        counter = 0\n","        while True:\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            if counter == target:     # To get desired fps for videocapture\n","              frame = crop_center_square(frame)\n","              frame = cv2.resize(frame, resize)\n","              frame = frame[:, :, [2, 1, 0]]\n","              frames.append(frame)\n","\n","              if len(frames) == max_frames:\n","                  break\n","              \n","              counter = 0   #reset fps counter\n","\n","            else:\n","              counter += 1\n","\n","    finally:\n","        cap.release()\n","    return np.array(frames)\n"]},{"cell_type":"markdown","source":["## Defining Feature Extractor\n","\n","We have used MobileNetV2 model to extract meaningful features from the extracted frames. This is a state-of-the-art model pre-trained on the ImageNet-1k dataset."],"metadata":{"id":"JxY0mPk8X_JS"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2397,"status":"ok","timestamp":1642145866190,"user":{"displayName":"CHINMAY INGLE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJoPoknO1Net5LVpEE3dmLs8dT4g8BSLE7zj2H=s64","userId":"10102245642602339447"},"user_tz":-330},"id":"FH6Smy7FoP2S","outputId":"0036db18-902b-46d8-f306-fb05694dd1b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n","9412608/9406464 [==============================] - 0s 0us/step\n","9420800/9406464 [==============================] - 0s 0us/step\n"]}],"source":["\n","def build_feature_extractor():\n","    feature_extractor = keras.applications.mobilenet_v2.MobileNetV2(\n","        weights=\"imagenet\",\n","        include_top=False,\n","        pooling=\"avg\",\n","        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n","    )\n","    preprocess_input = keras.applications.mobilenet_v2.preprocess_input\n","\n","    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n","    preprocessed = preprocess_input(inputs)\n","\n","    outputs = feature_extractor(preprocessed)\n","    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n","\n","\n","feature_extractor = build_feature_extractor()   # Returns CNN model to feature_extractor"]},{"cell_type":"markdown","metadata":{"id":"T1wTOD-YNvwA"},"source":["Encoding class labels"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":440,"status":"ok","timestamp":1642271217337,"user":{"displayName":"CHINMAY INGLE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJoPoknO1Net5LVpEE3dmLs8dT4g8BSLE7zj2H=s64","userId":"10102245642602339447"},"user_tz":-330},"id":"9jF7uZaaoP2S","outputId":"2fb06cd3-4a5a-4750-d4f9-da5df688ac58"},"outputs":[{"output_type":"stream","name":"stdout","text":["['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress', 'Biking', 'Billiards', 'BlowDryHair', 'BlowingCandles', 'BodyWeightSquats', 'Bowling', 'BoxingPunchingBag', 'BoxingSpeedBag', 'BreastStroke', 'BrushingTeeth', 'CleanAndJerk', 'CliffDiving', 'CricketBowling', 'CricketShot', 'CuttingInKitchen', 'Diving', 'Drumming', 'Fencing', 'FieldHockeyPenalty', 'FloorGymnastics', 'FrisbeeCatch', 'FrontCrawl', 'GolfSwing', 'Haircut', 'HammerThrow', 'Hammering', 'HandstandPushups', 'HandstandWalking', 'HeadMassage', 'HighJump', 'HorseRace', 'HorseRiding', 'HulaHoop', 'IceDancing', 'JavelinThrow', 'JugglingBalls', 'JumpRope', 'JumpingJack', 'Kayaking', 'Knitting', 'LongJump', 'Lunges', 'MilitaryParade', 'Mixing', 'MoppingFloor', 'Nunchucks', 'ParallelBars', 'PizzaTossing', 'PlayingCello', 'PlayingDaf', 'PlayingDhol', 'PlayingFlute', 'PlayingGuitar', 'PlayingPiano', 'PlayingSitar', 'PlayingTabla', 'PlayingViolin', 'PoleVault', 'PommelHorse', 'PullUps', 'Punch', 'PushUps', 'Rafting', 'RockClimbingIndoor', 'RopeClimbing', 'Rowing', 'SalsaSpin', 'ShavingBeard', 'Shotput', 'SkateBoarding', 'Skiing', 'Skijet', 'SkyDiving', 'SoccerJuggling', 'SoccerPenalty', 'StillRings', 'SumoWrestling', 'Surfing', 'Swing', 'TableTennisShot', 'TaiChi', 'TennisSwing', 'ThrowDiscus', 'TrampolineJumping', 'Typing', 'UnevenBars', 'VolleyballSpiking', 'WalkingWithDog', 'WallPushups', 'WritingOnBoard', 'YoYo']\n"]}],"source":["# Get class names\n","label_processor = keras.layers.StringLookup(\n","    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"])\n",")\n","class_names = label_processor.get_vocabulary()\n","print(class_names)\n","\n","# save class names\n","with open('class_name_list.pickle', 'wb') as f:\n","  pickle.dump(class_names, f)\n","!cp class_name_list.pickle /content/gdrive/MyDrive"]},{"cell_type":"markdown","metadata":{"id":"ShVT2fKKoP2T"},"source":["Finally, we can put all the pieces together to create our data processing utility."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_U4A6_-4oP2T"},"outputs":[],"source":["\n","def prepare_all_videos(df, root_dir):\n","    num_samples = len(df)\n","    video_paths = df[\"video_name\"].values.tolist()\n","    labels = df[\"tag\"].values\n","    labels = label_processor(labels[..., None]).numpy()\n","\n","    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n","    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n","    # masked with padding or not.\n","    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n","    frame_features = np.zeros(\n","        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n","    )\n","\n","    # For each video.\n","    for idx, path in enumerate(video_paths):\n","        # Gather all its frames and add a batch dimension.\n","        frames = load_video(os.path.join(root_dir, path))\n","        frames = frames[None, ...]\n","\n","        # Initialize placeholders to store the masks and features of the current video.\n","        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n","        temp_frame_features = np.zeros(\n","            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n","        )\n","\n","        # Extract features from the frames of the current video.\n","        for i, batch in enumerate(frames):\n","            video_length = batch.shape[0]\n","            length = min(MAX_SEQ_LENGTH, video_length)\n","            for j in range(length):\n","                temp_frame_features[i, j, :] = feature_extractor.predict(\n","                    batch[None, j, :]\n","                )\n","            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n","\n","        frame_features[idx,] = temp_frame_features.squeeze()\n","        frame_masks[idx,] = temp_frame_mask.squeeze()\n","\n","    return (frame_features, frame_masks), labels"]},{"cell_type":"markdown","source":["### Extract Features"],"metadata":{"id":"dZIFiT2R80gp"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JPtKPnjt-dtu","outputId":"e070cf22-8e97-4d38-9d7a-7f86084d10a8","executionInfo":{"status":"ok","timestamp":1641989329735,"user_tz":-330,"elapsed":13234431,"user":{"displayName":"CHINMAY INGLE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJoPoknO1Net5LVpEE3dmLs8dT4g8BSLE7zj2H=s64","userId":"10102245642602339447"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Frame features in train set: (9537, 20, 1280)\n","Frame masks in train set: (9537, 20)\n"]}],"source":["train_data, train_labels = prepare_all_videos(train_df, \"train\")\n","\n","print(f\"Frame features in train set: {train_data[0].shape}\")\n","print(f\"Frame masks in train set: {train_data[1].shape}\")"]},{"cell_type":"markdown","source":["Save extracted features from train data"],"metadata":{"id":"ibwkorch1qEk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"PnndWvWFPueX"},"outputs":[],"source":["with open('train_data.pickle', 'wb') as f:\n","    pickle.dump(train_data, f)\n","\n","# Save train labels\n","with open('train_labels.pickle', 'wb') as f:\n","    pickle.dump(train_labels, f)\n","\n","# zip and save to drive\n","!tar cf train_features.tar.gz train_data.pickle train_labels.pickle\n","!cp train_features.tar.gz /content/gdrive/MyDrive"]},{"cell_type":"markdown","source":[" Extract features from test set"],"metadata":{"id":"9eynjarMofdg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kEsN7iNRyWZy"},"outputs":[],"source":["test_data, test_labels = prepare_all_videos(test_df, \"test\")"]},{"cell_type":"markdown","metadata":{"id":"zfRfre6UTJq9"},"source":["Save extracted features from test data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9eHqK4njPjnI"},"outputs":[],"source":["# Save extracted features from test data\n","with open('test_data.pickle', 'wb') as f:\n","    pickle.dump(test_data, f)\n","\n","# Save test labels\n","with open('test_labels.pickle', 'wb') as f:\n","    pickle.dump(test_labels, f)\n","\n","# zip and save to drive\n","!tar cf test_features.tar.gz test_data.pickle test_labels.pickle\n","!cp test_features.tar.gz /content/gdrive/MyDrive"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"File_2_Feature_extr.ipynb","provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/vision/ipynb/video_classification.ipynb","timestamp":1641376063010}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}